{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9fd554-75db-42c5-9493-81dd4679b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e1577c-f255-4f82-80e6-e5ee3c862f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0.0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ab1a12-cb12-466f-bef6-71ea00a0f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 1\n",
    "out_dim = 1\n",
    "hidden_dims = [2]          \n",
    "num_layers = len(hidden_dims) + 1\n",
    "\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, in_dim, out_dim, hidden_dims, key):\n",
    "        self.layer_dims = [in_dim] + hidden_dims + [out_dim]\n",
    "        self.num_layers = len(self.layer_dims) - 1\n",
    "        self.params = self.init_params(key)\n",
    "\n",
    "    def init_params(self, key):\n",
    "        params = []\n",
    "        keys = jax.random.split(key, self.num_layers)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            k = keys[i]\n",
    "            W = jax.random.normal(k, (self.layer_dims[i], self.layer_dims[i + 1]))\n",
    "            b = jnp.zeros((self.layer_dims[i + 1],))\n",
    "            params.append((W, b))\n",
    "\n",
    "        return params\n",
    "\n",
    "def forward(params, x, activation):\n",
    "    h = x\n",
    "    for i, (W, b) in enumerate(params):\n",
    "        z = h @ W + b\n",
    "        if i < len(params) - 1:\n",
    "            h = activation(z)\n",
    "        else:\n",
    "            h = z\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f61b22-4b30-409a-b16b-1a694e52ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params,  x, y_true, activation):\n",
    "    y_pred = forward(params, x, activation)\n",
    "    return jnp.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a2162d9-fb10-4b06-bbdf-af2378ff9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit,  static_argnames=(\"activation\",))\n",
    "\n",
    "def train_step(params, x, y, lr, activation):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, x, y, activation)\n",
    "\n",
    "    new_params = [\n",
    "        (W - lr * dW, b - lr * db)\n",
    "        for (W, b), (dW, db) in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe405bb-672a-4941-a1ab-12a1c0269f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(activation):\n",
    "    @jax.jit\n",
    "    def train_step_jit(params, x, y, lr):\n",
    "        loss, grads = jax.value_and_grad(mse_loss)(\n",
    "            params, x, y, activation\n",
    "        )\n",
    "        new_params = [\n",
    "            (W - lr * dW, b - lr * db)\n",
    "            for (W, b), (dW, db) in zip(params, grads)\n",
    "        ]\n",
    "        return new_params, loss\n",
    "\n",
    "    return train_step_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b11e48fb-036c-41f4-9a3b-3948ddc8dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3694694]\n",
      " [1.384792 ]\n",
      " [1.4001145]\n",
      " [1.4154371]\n",
      " [1.4307597]\n",
      " [1.4460824]\n",
      " [1.4614049]\n",
      " [1.4767275]\n",
      " [1.49205  ]\n",
      " [1.5073726]]\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "num_samples = 10\n",
    "\n",
    "# Input data\n",
    "x = jnp.linspace(0, 1, num_samples).reshape(-1, in_dim)\n",
    "\n",
    "# Random ground-truth linear model\n",
    "key = jax.random.PRNGKey(42)\n",
    "key_A, key_b = jax.random.split(key)\n",
    "\n",
    "A_true = jax.random.normal(key_A, (in_dim, out_dim))\n",
    "b_true = jax.random.normal(key_b, (out_dim,))\n",
    "\n",
    "# Generate targets\n",
    "y = x @ A_true + b_true\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b5c13e9-22c3-4ba0-9cfe-5619ac826e75",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_train_step() takes 1 positional argument but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2000\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     params, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmake_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: make_train_step() takes 1 positional argument but 5 were given"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "model = NeuralNetwork(in_dim, out_dim, hidden_dims, key)\n",
    "params = model.params\n",
    "activation = relu\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    params, loss = make_train_step(params, x, y, lr, activation)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9c584c-b75e-46a5-8672-4bf34d589d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.        ]\n",
      " [0.1230425 ]\n",
      " [0.246085  ]\n",
      " [0.36912754]\n",
      " [0.49217   ]\n",
      " [0.61521256]\n",
      " [0.7382551 ]\n",
      " [0.8612976 ]\n",
      " [0.98434   ]\n",
      " [1.1073825 ]]\n",
      "True targets: [[1.3694694]\n",
      " [1.384792 ]\n",
      " [1.4001145]\n",
      " [1.4154371]\n",
      " [1.4307597]\n",
      " [1.4460824]\n",
      " [1.4614049]\n",
      " [1.4767275]\n",
      " [1.49205  ]\n",
      " [1.5073726]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = forward(params, x, activation)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"True targets:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fa563-0124-46e6-9a0a-e43a108613b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b6a9a-ac3f-44a1-a177-a7e0a743aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95435cc-92d7-4f39-9bbc-3afb9b28676e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe33510-0a28-4f0e-a7ec-c89fbaea61ce",
   "metadata": {},
   "source": [
    "# Exercise 3a\n",
    "\n",
    "Generating the data and splitting it into 80/20 train/test datasets, where we shuffle to avoid the model from learning orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5a5af5a-814b-4f19-a619-6d0d509641cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# generate training data\n",
    "\n",
    "def training_data(k,N):\n",
    "    x = jnp.linspace(-1.0, 1.0, N).reshape(-1, 1)\n",
    "    y = jnp.sin(k * jnp.pi * x)\n",
    "    return x,y\n",
    "\n",
    "x, y = training_data(1,100)\n",
    "\n",
    "def train_test_split(x, y, test_ratio=0.2, key=jax.random.PRNGKey(0)):\n",
    "    N = x.shape[0]\n",
    "    perm = jax.random.permutation(key, N)\n",
    "\n",
    "    x_shuffled = x[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    test_size = int(test_ratio * N)\n",
    "\n",
    "    x_test = x_shuffled[:test_size]\n",
    "    y_test = y_shuffled[:test_size]\n",
    "\n",
    "    x_train = x_shuffled[test_size:]\n",
    "    y_train = y_shuffled[test_size:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5a8d3-7d4a-43a8-8e73-5164af004568",
   "metadata": {},
   "source": [
    "Training function using batch gradient descent, returning epoch losses and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a53acc7d-598d-4799-9822-518e119758e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(key, hidden_dims, activation, lr, batch_size, x_train, y_train, num_epochs=2000):\n",
    "    model = NeuralNetwork(\n",
    "        in_dim = 1,\n",
    "        out_dim = 1,\n",
    "        hidden_dims = hidden_dims, \n",
    "        key = key\n",
    "    )\n",
    "\n",
    "    params = model.params\n",
    "\n",
    "    num_samples = x_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "\n",
    "        x_shuffled = x_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "\n",
    "        total_loss = 0\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            xb = x_shuffled[i: i + batch_size]\n",
    "            yb = y_shuffled[i: i + batch_size]\n",
    "                \n",
    "            params, loss = train_step(\n",
    "                params, \n",
    "                xb, \n",
    "                yb, \n",
    "                lr, \n",
    "                activation\n",
    "            )\n",
    "            total_loss += loss\n",
    "        epoch_mse = mse_loss(params, x_train, y_train, activation)\n",
    "        epoch_losses.append(epoch_mse)\n",
    "\n",
    "    return params, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc55e45-9122-46ec-a6e0-517f6972674f",
   "metadata": {},
   "source": [
    "Implement n_folds cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96b3e011-28a6-476c-9760-84fa03ff2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(key, N, n_folds=5):\n",
    "    perm = jax.random.permutation(key, N)\n",
    "    return jnp.array_split(perm, n_folds)\n",
    "\n",
    "def cross_val_score(key, hidden_dims, activation, lr, batch_size, x_train, y_train, num_epochs, n_folds = 5):\n",
    "\n",
    "    N = x.shape[0]\n",
    "    folds = make_folds(key, N, n_folds)\n",
    "    val_losses = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = jnp.concatenate(\n",
    "            [folds[j] for j in range(n_folds) if j != i]\n",
    "        )\n",
    "\n",
    "        x_train = x[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        x_val = x[val_idx]\n",
    "        y_val = y[val_idx]\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "\n",
    "        params, loss = train_nn(\n",
    "            key=subkey,\n",
    "            hidden_dims=hidden_dims,\n",
    "            activation=activation,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "\n",
    "        val_loss = mse_loss(params, x_val, y_val, activation)\n",
    "        val_losses.append(val_loss)\n",
    "    return jnp.mean(jnp.array(val_losses))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ffeeb-da34-4593-ab50-0dca8f969fe1",
   "metadata": {},
   "source": [
    "Implementing grid search function and printing best configuration and associated MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06e39263-a175-4c41-93ee-c79a22d91df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyperparameter optimization for fitting f1\n",
    "# Hyperparameters to vary (Question 2a)\n",
    "\n",
    "param_grid = {\n",
    "    'architectures': [[128], [128, 64]],\n",
    "    'learning_rates': [0.1, 0.01],\n",
    "    'batch_sizes': [32],\n",
    "    'activations': [relu, sigmoid],\n",
    "    'init_type': [\"xavier\"]\n",
    "}\n",
    "    \n",
    "\n",
    "def grid_search_cv(\n",
    "    param_grid,\n",
    "    key,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    num_epochs=300,\n",
    "    n_folds=5\n",
    "):\n",
    "    best_loss = jnp.inf\n",
    "    best_config = None\n",
    "\n",
    "    keys = jax.random.split(\n",
    "        key,\n",
    "        len(param_grid['learning_rates'])\n",
    "        * len(param_grid['architectures'])\n",
    "        * len(param_grid['batch_sizes'])\n",
    "        * len(param_grid['activations'])\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for lr in param_grid['learning_rates']:\n",
    "        for arch in param_grid['architectures']:\n",
    "            for batch_size in param_grid['batch_sizes']:\n",
    "                for activation in param_grid['activations']:\n",
    "                    subkey = keys[idx]\n",
    "                    idx += 1\n",
    "\n",
    "                    cv_loss = cross_val_score(\n",
    "                        key=subkey,\n",
    "                        hidden_dims=arch,\n",
    "                        activation=activation,\n",
    "                        lr=lr,\n",
    "                        batch_size=batch_size,\n",
    "                        x_train=x_train,\n",
    "                        y_train=y_train,\n",
    "                        num_epochs=num_epochs,\n",
    "                        n_folds=n_folds\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        f\"lr={lr}, \"\n",
    "                        f\"arch={arch}, \"\n",
    "                        f\"batch_size={batch_size}, \"\n",
    "                        f\"activation={activation.__name__}, \"\n",
    "                        f\"CV MSE={cv_loss:.6f}\"\n",
    "                    )\n",
    "\n",
    "                    if cv_loss < best_loss:\n",
    "                        best_loss = cv_loss\n",
    "                        best_config = {\n",
    "                            'learning_rate': lr,\n",
    "                            'architecture': arch,\n",
    "                            'batch_size': batch_size,\n",
    "                            'activation': activation\n",
    "                        }\n",
    "\n",
    "    return best_config, best_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde0e84-574d-4bbc-9eef-278f6c13d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "x_train, y_train, x_test, y_test = train_test_split(x,y)\n",
    "\n",
    "best_config, best_loss = grid_search_cv(\n",
    "    param_grid=param_grid,\n",
    "    key=key,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    num_epochs=1500,\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(best_config)\n",
    "print(\"Best CV MSE:\", best_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327457f-c785-4d5f-b531-662b8d21b1ee",
   "metadata": {},
   "source": [
    "Using the best hyperparameters to train the NN and plotting the predictions versus the true function. Note that since we took random test points in $[-1,1]$, the plot does not look like the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b19e2d2a-734a-4ae2-947a-9fa036219026",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_params, best_epoch_losses \u001b[38;5;241m=\u001b[39m train_nn(key, \u001b[43mbest_config\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m],best_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m], best_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], best_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], x_train, y_train, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m forward(\n\u001b[0;32m      4\u001b[0m     best_params,\n\u001b[0;32m      5\u001b[0m     x_test,\n\u001b[0;32m      6\u001b[0m     best_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m test_mse \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y_test) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_config' is not defined"
     ]
    }
   ],
   "source": [
    "best_params, best_epoch_losses = train_nn(key, best_config['architecture'],best_config['activation'], best_config['learning_rate'], best_config['batch_size'], x_train, y_train, num_epochs=2000)\n",
    "\n",
    "y_pred = forward(\n",
    "    best_params,\n",
    "    x_test,\n",
    "    best_config['activation']\n",
    ")\n",
    "\n",
    "test_mse = jnp.mean((y_pred - y_test) ** 2)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "\n",
    "\n",
    "# sort test points for a clean plot\n",
    "idx = jnp.argsort(x_test[:, 0])\n",
    "x_sorted = x_test[idx]\n",
    "y_true_sorted = y_test[idx]\n",
    "y_pred_sorted = y_pred[idx]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_sorted, y_true_sorted, label=\"True function\", linewidth=2)\n",
    "plt.plot(x_sorted, y_pred_sorted, \"--\", label=\"Model prediction\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Model prediction vs true function\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a47a1a-f586-4d92-a48d-e7a34b4cfa28",
   "metadata": {},
   "source": [
    "# Exercise 3b)\n",
    "\n",
    "The grid search with 5-fold cross validation outputs optimal parameters: \n",
    "For a large learning rate of 0.1, we see that the neural network architecture of two hidden layers with 128, 64 dimensions outputs NaN values. We see that this problem doesnt appear for lower learning rates which indicates that the gradient might be the problem. Due to the multiplication of the learning rate with the gradient, the gradient might explode leading to NaNs.\n",
    "For smaller learning rates this architecture actually performs better than the shallow neural network. This may be due to the flexibility of the neural network, as the increasement of weights allows for more learning capacity. \n",
    "\n",
    "The following is for k = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97285a-0c0d-4239-8135-856b1a831db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01, arch=[128], batch_size=32, activation=relu, CV MSE=0.000702\n",
      "lr=0.01, arch=[128], batch_size=32, activation=sigmoid, CV MSE=0.022855\n",
      "lr=0.01, arch=[128, 64], batch_size=32, activation=relu, CV MSE=nan\n",
      "lr=0.01, arch=[128, 64], batch_size=32, activation=sigmoid, CV MSE=0.001967\n",
      "lr=0.001, arch=[128], batch_size=32, activation=relu, CV MSE=0.019760\n",
      "lr=0.001, arch=[128], batch_size=32, activation=sigmoid, CV MSE=0.153035\n",
      "lr=0.001, arch=[128, 64], batch_size=32, activation=relu, CV MSE=0.004252\n",
      "lr=0.001, arch=[128, 64], batch_size=32, activation=sigmoid, CV MSE=0.016322\n",
      "\n",
      "Best hyperparameters:\n",
      "{'learning_rate': 0.01, 'architecture': [128], 'batch_size': 32, 'activation': <function relu at 0x00000155BE23F520>}\n",
      "Best CV MSE: 0.0007018956\n",
      "Test MSE: 0.10310117\n"
     ]
    }
   ],
   "source": [
    "# generate training data varying k \n",
    "def generate_data_k(k, N):\n",
    "    xk, yk = training_data(k, N)\n",
    "    xk_train, yk_train, xk_test, yk_test = train_test_split(xk, yk)\n",
    "    return xk_train, yk_train, xk_test, yk_test\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "param_grid = {\n",
    "    'architectures': [[128], [128, 64]],\n",
    "    'learning_rates': [0.01, 0.001],\n",
    "    'batch_sizes': [32],\n",
    "    'activations': [relu, sigmoid],\n",
    "    'init_type': [\"xavier\"]\n",
    "}\n",
    "\n",
    "def find_best_config(param_grid, xk_train, yk_train, key):\n",
    "    # find best configuration \n",
    "    best_config, best_loss = grid_search_cv(\n",
    "        param_grid=param_grid,\n",
    "        key=key,\n",
    "        x_train=xk_train,\n",
    "        y_train=yk_train,\n",
    "        num_epochs=1500,\n",
    "        n_folds=5\n",
    "    )\n",
    "\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    print(best_config)\n",
    "    print(\"Best CV MSE:\", best_loss)\n",
    "    return best_config, best_loss\n",
    "\n",
    "def predict_and_plot_test(best_config, xk_train, yk_train, xk_test, yk_test, key):\n",
    "    best_params, best_epoch_losses = train_nn(key, best_config['architecture'],best_config['activation'], best_config['learning_rate'], best_config['batch_size'], xk_train, yk_train, num_epochs=2000)\n",
    "\n",
    "    x_plot = jnp.linspace(-1.0, 1.0, 2000).reshape(-1, 1)\n",
    "\n",
    "    y_pred = forward(\n",
    "        best_params,\n",
    "        xk_test,\n",
    "        best_config['activation']\n",
    "    )\n",
    "\n",
    "    test_mse = jnp.mean((y_pred - yk_test) ** 2)\n",
    "    print(\"Test MSE:\", test_mse)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # sort test points for a clean plot\n",
    "    idx = jnp.argsort(xk_test[:, 0])\n",
    "    x_sorted = xk_test[idx]\n",
    "    y_true_sorted = yk_test[idx]\n",
    "    y_pred_sorted = y_pred[idx]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x_sorted, y_true_sorted, label=\"True function\", linewidth=2)\n",
    "    plt.plot(x_sorted, y_pred_sorted, \"--\", label=\"Model prediction\", linewidth=2)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Model prediction vs true function\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return test_mse\n",
    "\n",
    "x3_train, y3_train, x3_test, y3_test = generate_data_k(3, 1000)\n",
    "best_config_3, best_loss_3 =  find_best_config(param_grid, x3_train, y3_train, key)\n",
    "test_mse_3 = predict_and_plot_test(best_config_3, x3_train, y3_train, x3_test, y3_test, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b2e93-817e-4b8a-9ec9-0bcdbccc426b",
   "metadata": {},
   "source": [
    "## k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06da1a-3df2-4d6f-8b7a-6cc620158589",
   "metadata": {},
   "outputs": [],
   "source": [
    "x5_train, y5_train, x5_test, y5_test = generate_data_k(5, 1000)\n",
    "best_config_5, best_loss_5 =  find_best_config(param_grid, x5_train, y5_train, key)\n",
    "test_mse_5 = predict_and_plot_test(best_config_5, x5_train, y5_train, x5_test, y5_test, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ce98c-1657-48c5-a641-de56ca777d68",
   "metadata": {},
   "source": [
    "## k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e888c1-fe61-4d05-9b7a-4e106cff0d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67566221-a8f1-478c-9899-5840b4453fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax)",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
