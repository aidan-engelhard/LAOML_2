{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f131f2e7",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12d1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1eac0",
   "metadata": {},
   "source": [
    "ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755a89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dff27",
   "metadata": {},
   "source": [
    "Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57481487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56be67",
   "metadata": {},
   "source": [
    "Flexible neural network architecture with initial random weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b576e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 1\n",
    "out_dim = 1\n",
    "hidden_dims = [2]          \n",
    "num_layers = len(hidden_dims) + 1\n",
    "\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, in_dim, out_dim, hidden_dims, key, init_type=\"standard\"):\n",
    "        \"\"\"\n",
    "        init_type: \"standard\" for N(0, 1) or \"xavier\" for Xavier/Glorot weighting\n",
    "        \"\"\"\n",
    "        self.layer_dims = [in_dim] + hidden_dims + [out_dim]\n",
    "        self.num_layers = len(self.layer_dims) - 1\n",
    "        self.init_type = init_type # Store the initialization choice\n",
    "\n",
    "        self.params = self.init_params(key)\n",
    "\n",
    "    def init_params(self, key):\n",
    "        params = []\n",
    "        keys = jax.random.split(key, self.num_layers)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            k = keys[i]\n",
    "            n_in = self.layer_dims[i]\n",
    "            n_out = self.layer_dims[i + 1]\n",
    "            \n",
    "            # Draw from standard normal distribution N(0, 1)\n",
    "            W = jax.random.normal(k, (n_in, n_out))\n",
    "            \n",
    "            # Apply weighting based on choice \n",
    "            if self.init_type == \"xavier\":\n",
    "                stddev = jnp.sqrt(2.0 / (n_in + n_out))\n",
    "                W = W * stddev\n",
    "            # If init_type is \"standard\", W remains N(0, 1)\n",
    "            \n",
    "            b = jnp.zeros((n_out,))\n",
    "            params.append((W, b))\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "def forward(params, x, activation):\n",
    "    h = x\n",
    "    for i, (W, b) in enumerate(params):\n",
    "        z = h @ W + b\n",
    "        if i < len(params) - 1:\n",
    "            h = activation(z)\n",
    "        else:\n",
    "            h = z\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a752fe",
   "metadata": {},
   "source": [
    "MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f6483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params,  x, y_true, activation):\n",
    "    y_pred = forward(params, x, activation)\n",
    "    return jnp.mean((y_pred - y_true) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292636c",
   "metadata": {},
   "source": [
    "Training step using automatic differentation from JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fce4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit,  static_argnames=(\"activation\",))\n",
    "def train_step(params, x, y, lr, activation):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, x, y, activation)\n",
    "\n",
    "    new_params = [\n",
    "        (W - lr * dW, b - lr * db)\n",
    "        for (W, b), (dW, db) in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bc830",
   "metadata": {},
   "source": [
    "Simplest initial example, $ y = Ax + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4aa937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60576403]\n",
      " [0.6142002 ]\n",
      " [0.6226364 ]\n",
      " [0.6310725 ]\n",
      " [0.6395087 ]\n",
      " [0.64794487]\n",
      " [0.6563811 ]\n",
      " [0.6648172 ]\n",
      " [0.6732534 ]\n",
      " [0.68168956]]\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "num_samples = 10\n",
    "\n",
    "# Input data\n",
    "x = jnp.linspace(0, 1, num_samples).reshape(-1, in_dim)\n",
    "\n",
    "# Random ground-truth linear model\n",
    "key = jax.random.PRNGKey(42)\n",
    "key_A, key_b = jax.random.split(key)\n",
    "\n",
    "A_true = jax.random.normal(key_A, (in_dim, out_dim))\n",
    "b_true = jax.random.normal(key_b, (out_dim,))\n",
    "\n",
    "# Generate targets\n",
    "y = x @ A_true + b_true\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02267602",
   "metadata": {},
   "source": [
    "Initialise and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63371d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.385397374630\n",
      "epoch 100, loss: 0.000082268067\n",
      "epoch 200, loss: 0.000031653235\n",
      "epoch 300, loss: 0.000012800811\n",
      "epoch 400, loss: 0.000005217569\n",
      "epoch 500, loss: 0.000002131869\n",
      "epoch 600, loss: 0.000000872130\n",
      "epoch 700, loss: 0.000000357049\n",
      "epoch 800, loss: 0.000000146241\n",
      "epoch 900, loss: 0.000000059920\n",
      "epoch 1000, loss: 0.000000024555\n",
      "epoch 1100, loss: 0.000000010062\n",
      "epoch 1200, loss: 0.000000004128\n",
      "epoch 1300, loss: 0.000000001692\n",
      "epoch 1400, loss: 0.000000000694\n",
      "epoch 1500, loss: 0.000000000284\n",
      "epoch 1600, loss: 0.000000000116\n",
      "epoch 1700, loss: 0.000000000048\n",
      "epoch 1800, loss: 0.000000000020\n",
      "epoch 1900, loss: 0.000000000008\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "model = NeuralNetwork(in_dim, out_dim, hidden_dims, key)\n",
    "params = model.params\n",
    "activation = relu\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    params, loss = train_step(params, x, y, lr, activation)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ad4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.605766  ]\n",
      " [0.61419713]\n",
      " [0.622634  ]\n",
      " [0.63107085]\n",
      " [0.6395077 ]\n",
      " [0.64794457]\n",
      " [0.6563814 ]\n",
      " [0.6648183 ]\n",
      " [0.6732552 ]\n",
      " [0.68169206]]\n",
      "True targets: [[0.60576403]\n",
      " [0.6142002 ]\n",
      " [0.6226364 ]\n",
      " [0.6310725 ]\n",
      " [0.6395087 ]\n",
      " [0.64794487]\n",
      " [0.6563811 ]\n",
      " [0.6648172 ]\n",
      " [0.6732534 ]\n",
      " [0.68168956]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = forward(params, x, activation)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"True targets:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae31a0c",
   "metadata": {},
   "source": [
    "Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07a9dc-7942-4465-ac32-44e34dd5daba",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "In accordance with the project suggestions, we use the `scikit-learn` library to retrieve the MNIST dataset. This dataset contains 70,000 grayscale images of handwritten digits (0-9), each of size $28 \\times 28$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad2d42c-26a6-44a4-8bc7-86bdcc096de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Dataset loaded: 70000 samples with 784 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# Fetching the MNIST dataset (784 features correspond to 28x28 flattened pixels)\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False) \n",
    "\n",
    "# X contains the flattened vector representations of the images\n",
    "# y contains the corresponding labels (0-9)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.uint8) \n",
    "\n",
    "print(f\"Dataset loaded: {X.shape[0]} samples with {X.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ca6d6-f754-4842-8cd4-5b02ae89d5ad",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "To better understand the dataset we are working with, the following cell implements a function to visualize individual MNIST images from their flattened vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2896268-c7a6-4836-8ee1-c50be40b7f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(data, label):\n",
    "    \"\"\"\n",
    "    Visualizes a single MNIST digit.\n",
    "    Reshapes the 784 flattened vector back to a 28x28 image.\n",
    "    \"\"\"\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example: visualizing the first image of the dataset\n",
    "plot_digit(X[789], y[789])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24730a4-1791-4433-a199-7dc2c6f70189",
   "metadata": {},
   "source": [
    "## 2.a. Training of the neural network with cross-entropy loss and mini-batch gradient descent.\n",
    "\n",
    "#### Softmax\n",
    "To handle the multiclass classification of MNIST digits, we implement the **Softmax** function to transform the network's raw outputs into probability distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a900da-b144-415b-837f-1de17812db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # Subtraction of max improves numerical stability\n",
    "    e_x = jnp.exp(x - jnp.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / jnp.sum(e_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e25dc-9b6e-4660-90a7-e8616983bd1e",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "Since the MNIST labels are integers from 0 to 9, we need to transform them into binary vectors of size 10. This **one-hot encoding** allows the network to compare its 10-way output probabilities with the ground truth during the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0337fd29-8505-428d-bef9-253a8be003ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead1d8a-f041-446d-813f-78e927c684e0",
   "metadata": {},
   "source": [
    "#### Loss Function \n",
    "We then define the **Categorical Cross-Entropy Loss**, which is the standard objective function for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b3ddbd-b1e0-4e4e-8d85-2fb6dd9b45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(params, x, y_true, activation):\n",
    "    \"\"\"Multiclass Cross-Entropy Loss.\"\"\"\n",
    "    # Forward pass through the network\n",
    "    logits = forward(params, x, activation)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    # Compute cross-entropy: -sum(y_true * log(probs))\n",
    "    # We add a small epsilon (1e-9) to avoid log(0)\n",
    "    return -jnp.mean(jnp.sum(y_true * jnp.log(probs + 1e-9), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8534b3-c2dd-4db0-9b9f-22d0ab6bd59b",
   "metadata": {},
   "source": [
    "#### Accuracy Metric\n",
    "While the loss function is used for optimization, **accuracy** provides a human-readable performance metric. It calculates the percentage of images correctly classified by comparing the index of the highest predicted probability with the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72498da8-28c4-4971-a775-ae048bcd3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, x, y_true, activation):\n",
    "    \"\"\"Computes the accuracy of the model.\"\"\"\n",
    "    logits = forward(params, x, activation)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(predictions == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85e044-0221-4e29-9e4b-02c5abe7404d",
   "metadata": {},
   "source": [
    "#### Data Normalization\n",
    "The raw pixel values in `X` range from 0 to 255. To ensure numerical stability and prevent gradients from vanishing or exploding during the training of our `NeuralNetwork`, we scale these values to the range $[0, 1]$ by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a722f557-8dd9-4b16-b0b1-04f47801ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original range: 0 to 255\n",
      "Normalized range: 0.0 to 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the feature matrix\n",
    "X_normalized = X / 255.0\n",
    "\n",
    "print(f\"Original range: {X.min()} to {X.max()}\")\n",
    "print(f\"Normalized range: {X_normalized.min()} to {X_normalized.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134bce4-6093-4782-9e26-a68104b4baa7",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "We split our dataset into a training set (to optimize the parameters) and a testing set (to evaluate final performance). We use 80% of the data for training and 20% for testing. We also convert these arrays into `jax.numpy` arrays to leverage JAX's computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4ab634a-d876-4bdc-b9ee-e193b4cc8dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pool (X_train_full): 56000 images\n",
      "Final test set (X_test): 14000 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Splitting into 80% train/validation pool and 20% final test\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_normalized, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Converting to JAX arrays \n",
    "# Note: We use the suffix '_full' to indicate this is the complete training pool \n",
    "# that the K-Fold will later split into training and validation folds.\n",
    "X_train_full = jnp.array(X_train_raw)\n",
    "X_test = jnp.array(X_test_raw)\n",
    "y_train_full = jnp.array(y_train_raw)\n",
    "y_test = jnp.array(y_test_raw)\n",
    "\n",
    "print(f\"Training pool (X_train_full): {X_train_full.shape[0]} images\")\n",
    "print(f\"Final test set (X_test): {X_test.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3b5c4-dcd3-4164-b3c0-3f7831ef86b1",
   "metadata": {},
   "source": [
    "#### k-fold cross-validation\n",
    "As requested in the project instructions, we implement a **k-fold cross-validation** ($k=5$) combined with a **grid search** to select the best hyperparameters. This approach ensures that our model's performance is not dependent on a specific train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbf7bdbc-5b78-4c9e-ae46-1598e4e010d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete for 5-fold cross-validation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Defining K=5 folds as suggested in the instructions\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Setup complete for {num_folds}-fold cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdc7c2-e55a-4f72-8dac-dd9151ad04be",
   "metadata": {},
   "source": [
    "## 2.b. Hyperparameters\n",
    "We define the grid of hyperparameters to explore, including hidden layer architectures, learning rates, and batch sizes, as specified in the assignment[cite: 31, 32]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7392038-8b49-4a05-abd0-483551fb467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to vary (Question 2a)\n",
    "param_grid = {\n",
    "    'learning_rates': [0.1, 0.01],\n",
    "    'architectures': [[128], [128, 64]],\n",
    "    'batch_sizes': [32, 64],\n",
    "    'epochs': [10] # Number of epochs for each fold\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005deeb-f55a-4292-b904-68185cc23df9",
   "metadata": {},
   "source": [
    "#### Training Step for Classification\n",
    "We update our training step to handle the multiclass classification task. Unlike the previous version used for regression, this function utilizes the **Categorical Cross-Entropy Loss**. It is decorated with `@jax.jit` to compile the function for faster execution and uses `jax.value_and_grad` to automatically compute the gradients of the loss with respect to the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba66a89c-36cf-4512-b088-20a6ca8d852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=(\"activation\",))\n",
    "def train_step_classification(params, x_batch, y_batch, lr, activation):\n",
    "    # Calcul de la perte et des gradients sur le batch actuel\n",
    "    loss, grads = jax.value_and_grad(cross_entropy_loss)(params, x_batch, y_batch, activation)\n",
    "    \n",
    "    # Mise à jour des paramètres (SGD)\n",
    "    new_params = [\n",
    "        (W - lr * dW, b - lr * db)\n",
    "        for (W, b), (dW, db) in zip(params, grads)\n",
    "    ]\n",
    "    return new_params, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ef4d8-75fb-4f27-89b5-a23b0dfd0ec2",
   "metadata": {},
   "source": [
    "#### Mini-batch Gradient Descent Implementation\n",
    "To follow the project requirements, we implement a training loop that processes data in mini-batches. This approach provides a balance between the stability of full-batch gradient descent and the efficiency of stochastic gradient descent. We also include a shuffling mechanism at each epoch to ensure the model does not learn the order of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7b77b18-f469-4b55-8dbb-684129ab0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_batches(X_train, y_train, config, key):\n",
    "    \"\"\"\n",
    "    Trains the network using mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    # 1. Initialization\n",
    "    model = NeuralNetwork(\n",
    "        in_dim=784, \n",
    "        out_dim=10, \n",
    "        hidden_dims=config['architecture'], \n",
    "        key=key, \n",
    "        init_type=config['init_type']\n",
    "    )\n",
    "    params = model.params\n",
    "    y_train_oh = one_hot(y_train, 10) # Format labels for Cross-Entropy\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    batch_size = config['batch_size']\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    # Track losses for the learning curves requested in Question 2a\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Shuffle data at the start of each epoch\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perm = jax.random.permutation(subkey, num_samples)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled_oh = y_train_oh[perm]\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            # Update parameters using only the current batch\n",
    "            params, loss = train_step_classification(\n",
    "                params, \n",
    "                X_shuffled[start:end], \n",
    "                y_shuffled_oh[start:end], \n",
    "                config['lr'], \n",
    "                config['activation']\n",
    "            )\n",
    "            total_loss += loss\n",
    "            \n",
    "        avg_loss = total_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "    return params, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8cc52-8817-4b75-a7e8-4ff5a38e93ab",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization Strategy\n",
    "To comply with the requirement of selecting \"promising hyperparameters\", we implement a nested loop. The outer loop iterates through our **Grid Search** configurations (architecture, learning rate, batch size), while the inner loop performs a **5-fold Cross-Validation**. This process ensures that the reported accuracy is an average across different data splits, providing a reliable measure of the model's generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5d3adbb-e626-4652-81d9-cc55c25596cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search: 8 combinations x 5 folds.\n",
      "\n",
      "Testing Config: Arch=[128], LR=0.1, Batch=32\n",
      "  -> Average Val Accuracy: 0.9747\n",
      "\n",
      "Testing Config: Arch=[128], LR=0.1, Batch=64\n",
      "  -> Average Val Accuracy: 0.9705\n",
      "\n",
      "Testing Config: Arch=[128], LR=0.01, Batch=32\n",
      "  -> Average Val Accuracy: 0.9428\n",
      "\n",
      "Testing Config: Arch=[128], LR=0.01, Batch=64\n",
      "  -> Average Val Accuracy: 0.9268\n",
      "\n",
      "Testing Config: Arch=[128, 64], LR=0.1, Batch=32\n",
      "  -> Average Val Accuracy: 0.9758\n",
      "\n",
      "Testing Config: Arch=[128, 64], LR=0.1, Batch=64\n",
      "  -> Average Val Accuracy: 0.9737\n",
      "\n",
      "Testing Config: Arch=[128, 64], LR=0.01, Batch=32\n",
      "  -> Average Val Accuracy: 0.9571\n",
      "\n",
      "Testing Config: Arch=[128, 64], LR=0.01, Batch=64\n",
      "  -> Average Val Accuracy: 0.9410\n",
      "\n",
      "--- Grid Search Complete ---\n",
      "Best Config found: {'architecture': [128, 64], 'lr': 0.1, 'batch_size': 32, 'epochs': 10, 'activation': <function relu at 0x15b31a700>, 'init_type': 'xavier'}\n",
      "Best Validation Accuracy: 0.9758\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the hyperparameter grid as specified in the project \n",
    "param_grid = {\n",
    "    'architecture': [[128], [128, 64]],\n",
    "    'lr': [0.1, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10],\n",
    "    'activation': [relu],\n",
    "    'init_type': [\"xavier\"]\n",
    "}\n",
    "\n",
    "# Generate all unique combinations from the grid\n",
    "keys, values = zip(*param_grid.items())\n",
    "grid_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "best_config = None\n",
    "best_avg_acc = -1.0\n",
    "best_params = None\n",
    "results_log = []\n",
    "\n",
    "print(f\"Starting Grid Search: {len(grid_combinations)} combinations x {num_folds} folds.\\n\")\n",
    "\n",
    "# 2. Outer Loop: Iterate through each hyperparameter configuration \n",
    "for config in grid_combinations:\n",
    "    fold_accuracies = []\n",
    "    print(f\"Testing Config: Arch={config['architecture']}, LR={config['lr']}, Batch={config['batch_size']}\")\n",
    "    \n",
    "    # 3. Inner Loop: Perform K-Fold Cross-Validation (K=5) \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "        # Split the training pool into training and validation folds for this iteration\n",
    "        X_fold_train, X_fold_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "        \n",
    "        # Train the model using the mini-batch gradient descent function [cite: 30]\n",
    "        key = jax.random.PRNGKey(fold) \n",
    "        params, _ = train_model_batches(X_fold_train, y_fold_train, config, key)\n",
    "        \n",
    "        # Evaluate performance on the validation fold\n",
    "        val_acc = accuracy(params, X_fold_val, y_fold_val, config['activation'])\n",
    "        fold_accuracies.append(val_acc)\n",
    "    \n",
    "    # Calculate the average accuracy across all 5 folds for this configuration\n",
    "    avg_acc = np.mean(fold_accuracies)\n",
    "    results_log.append({'config': config, 'avg_acc': avg_acc})\n",
    "    print(f\"  -> Average Val Accuracy: {avg_acc:.4f}\\n\")\n",
    "    \n",
    "    # Save the best configuration and parameters found so far\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_config = config\n",
    "        best_params = params # Prevents 'best_params not defined' error later\n",
    "\n",
    "print(\"--- Grid Search Complete ---\")\n",
    "print(f\"Best Config found: {best_config}\")\n",
    "print(f\"Best Validation Accuracy: {best_avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa25b0-0eff-47e7-9c6e-981cc3c23be6",
   "metadata": {},
   "source": [
    "### 2.c. Analysis of Misclassified Images\n",
    "After training our best model, we evaluate it on the test set to identify images where the prediction does not match the true label. Visualizing these errors helps us understand if the mistakes are due to ambiguous handwriting or inherent limitations of our dense network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0188b99-3a89-4589-bc3c-fe0862cca816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified images: 354 out of 14000\n"
     ]
    }
   ],
   "source": [
    "# 1. Get predictions on the test set\n",
    "test_logits = forward(best_params, X_test, relu) # Use your best activation\n",
    "test_predictions = jnp.argmax(test_logits, axis=-1)\n",
    "\n",
    "# 2. Find indices where predictions differ from true labels\n",
    "# Note: y_test is already jnp.array from our split step\n",
    "misclassified_indices = jnp.where(test_predictions != y_test)[0]\n",
    "\n",
    "print(f\"Number of misclassified images: {len(misclassified_indices)} out of {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92fe1-5971-44d7-8249-7eb91526f959",
   "metadata": {},
   "source": [
    "#### Visualization of Misclassified Images (Question 2c)\n",
    "To analyze the model's performance, we identify instances in the test set where the predicted label differs from the ground truth. Visualizing these 10 examples allows us to discuss possible reasons for failure, such as ambiguous handwriting or the limitations of flattened vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24125176-1570-4312-afad-1818d75b7d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misclassified in test set: 354 / 14000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Generate predictions for the entire test set using the best parameters\n",
    "# We use the activation function defined in our best_config\n",
    "test_logits = forward(best_params, X_test, best_config['activation'])\n",
    "test_predictions = jnp.argmax(test_logits, axis=-1)\n",
    "\n",
    "# 2. Identify indices where the prediction is incorrect\n",
    "# y_test contains the true labels (0-9)\n",
    "misclassified_indices = jnp.where(test_predictions != y_test)[0]\n",
    "\n",
    "print(f\"Total misclassified in test set: {len(misclassified_indices)} / {len(y_test)}\")\n",
    "\n",
    "# 3. Plot 10 misclassified images\n",
    "num_to_show = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(num_to_show):\n",
    "    idx = misclassified_indices[i]\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = test_predictions[idx]\n",
    "    \n",
    "    plt.subplot(1, num_to_show, i + 1)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\", color='red')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e83b3-8380-4e50-9d25-ad608020e5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
